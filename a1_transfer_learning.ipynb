{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[]},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":821742,"sourceType":"datasetVersion","datasetId":432700}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Transfer Learning with Pre-trained Models [3 points]\nUse in-build models with pre-trained weights and apply them to the Food-11 dataset.","metadata":{"id":"QFwbbmRcb284"}},{"cell_type":"code","source":"from torchvision import transforms\n\ntrain_transform = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.RandomHorizontalFlip(),\n    transforms.RandomRotation(10),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n])\n\nval_transform = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n])\n\ntest_transform = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n])\n","metadata":{"id":"fNHehnxpnQ7s","trusted":true,"execution":{"iopub.status.busy":"2025-03-07T15:04:49.243952Z","iopub.execute_input":"2025-03-07T15:04:49.244239Z","iopub.status.idle":"2025-03-07T15:04:53.072695Z","shell.execute_reply.started":"2025-03-07T15:04:49.244215Z","shell.execute_reply":"2025-03-07T15:04:53.072013Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"from torchvision import datasets\n\ntrain_path = '/kaggle/input/food11-image-dataset/training'\nval_path = '/kaggle/input/food11-image-dataset/validation'\ntest_path = '/kaggle/input/food11-image-dataset/evaluation'\n\ntrain_dataset = datasets.ImageFolder(root=train_path, transform=train_transform)\nval_dataset = datasets.ImageFolder(root=val_path, transform=val_transform)\ntest_dataset = datasets.ImageFolder(root=test_path, transform=test_transform)\n","metadata":{"id":"4gsvwRNBnSpS","trusted":true,"execution":{"iopub.status.busy":"2025-03-07T15:04:57.206107Z","iopub.execute_input":"2025-03-07T15:04:57.206649Z","iopub.status.idle":"2025-03-07T15:05:35.853040Z","shell.execute_reply.started":"2025-03-07T15:04:57.206614Z","shell.execute_reply":"2025-03-07T15:05:35.852428Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"from torch.utils.data import DataLoader\n\ntrain_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\ntest_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n\ndata_iter = iter(train_loader)\nimages, labels = next(data_iter)\n\nprint(\"Batch image size:\", images.size())  \nprint(\"Batch labels:\", labels) \n","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CPovJLIcnUt4","outputId":"a5bb30d5-3084-47f1-cf27-0490e2970d13","trusted":true,"execution":{"iopub.status.busy":"2025-03-07T15:07:12.827310Z","iopub.execute_input":"2025-03-07T15:07:12.827616Z","iopub.status.idle":"2025-03-07T15:07:13.274752Z","shell.execute_reply.started":"2025-03-07T15:07:12.827590Z","shell.execute_reply":"2025-03-07T15:07:13.273857Z"}},"outputs":[{"name":"stdout","text":"Batch image size: torch.Size([32, 3, 224, 224])\nBatch labels: tensor([ 4,  0,  1, 10,  6,  4,  0,  5,  0,  0,  0,  3,  2,  8,  7,  8,  0,  5,\n         9, 10,  2,  2,  2,  4,  9,  9,  4,  5,  5,  5,  2,  6])\n","output_type":"stream"}],"execution_count":3},{"cell_type":"markdown","source":"## Step 1: Select at least THREE different pre-trained models\nSelected at least THREE different pre-trained models, e.g. ShuffleNet, Inception V3, and MobileNet V3. Check PyTorch documentation for more details. Justify your choice of models, considering their architectural strengths and suitability for the task.","metadata":{"id":"Lgb7AF5Yb28_"}},{"cell_type":"markdown","source":"**Models choosen**\n----\n\n\n\n**1. ShuffleNet**\n\n\n###i. Efficiency: ShuffleNet was developed for efficiency. It implements group convolutions and channel shuffle operations that reduce computation while keeping accuracy relatively high.\n\n\n\n###ii. Low Latency: This characteristic does well on real-time applications or situations where minimum processing time is required. Good for applications involving mobile or embedded devices, it helps where you would want to deploy the model in such scenarios.\n\n\n\n###iii. Good for Image Classification: Even with its lightweight architecture, ShuffleNet performs very well concerning image classification tasks, including classification among many classes like Food-11\n\n**Reason why it is best for Food-11**\n###1. The dataset has a manageable number of categories, i.e. 11 food types, thus making it faster and efficiency classifier on the catering models given limited resources.\n###2. Use in applications where deployment is intended on a mobile device without much accuracy sacrifice..\n\n\n\n---\n\n\n\n**2. EfficientNet V2**\n\n###i. Squeeze and excitation blocks: This model uses squeeze and excitation block, which perfectly enhances the important feature representation by dynamically re-calibrating channel-wise feature responses. This helps in such cases when the object has been classified because it makes sure of the most relevant parts that make the images in the trained process focus on the most important attributes used for classification. This is specifically true within food classification applications due to textures and colors or fine details.\n\n###ii. High Accuracy: It uses Neural Architecture Search (NAS) to automagically surf model scaling and thus ensure the best possible depth-width-resolution trade-off for a particular dataset. It adapts its structure dynamically based on dataset requirements, leading to higher accuracy with fewer parameters.\n\n###iii. Fine tunes Well: This model is pre-trained extensively on data from large datasets such as ImageNet. As a result, it shows strong generalization ability toward food classification tasks. A structured feature extractor qualifies this model highly for transfer learning, in which less labeled data is needed for higher accuracy, making it extremely applicable for fine-tuning on specialized datasets like Food-11.\n\n**Reason why it is best for Food-11**\n###1.  Food recognition models generally result in real-time applications, like taking pictures of food for calorie tracking, stuff like menus in restaurants, and identifying food items. \n###2. This model is also very fast to classify images using light computing device.\n\n---\n\n\n\n**3. MobileNet V3**\n\n###i. Mobile Optimization: MobileNet V3 optimizes for mobile devices, which becomes a major advantage when model size and inference time become deciding factors.\n\n###ii. Efficient: MobileNet V3 applies depthwise separable convolutions to minimize parameters and computations. Therefore, compared to other networks, it is much lighter in weight without sacrificing much in accuracy.\n\n###iii. Good Tradeoff Between Accuracy and Speed: There is a tradeoff between an inference one can speed and one can accuracy, and MobileNet V3 is a good choice if both performance and efficiency are needed.\n\n**Reason why it is best for Food-11**\n###1. The diversity shown in Food-11 images allows MobileNet V3 design to generalize greatly on such types of datasets without increasing heavy computations\n###2. A great choice when you need to balance between accuracy and computational efficiency, especially for applications on mobile devices or systems with limited computational power","metadata":{"id":"osvwCII7b29G"}},{"cell_type":"markdown","source":"## Step 2: For each chosen model","metadata":{"id":"JLVTtSf_b29H"}},{"cell_type":"markdown","source":"### a. Load the pre-trained model and modify the classification head\nLoad the pre-trained model and modify the classification head (the final fully connected layer) to match the number of classes in the Food-11 dataset.","metadata":{"id":"hzLSYDdYb29H"}},{"cell_type":"markdown","source":"**ShuffleNet**","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom torchvision import models\n\nshufflenet = models.shufflenet_v2_x1_0(weights=models.ShuffleNet_V2_X1_0_Weights.IMAGENET1K_V1)\n\nfood_num_shuffle = shufflenet.fc.in_features\nshufflenet.fc = nn.Linear(food_num_shuffle, 11)  \n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nshufflenet = shufflenet.to(device)\n","metadata":{"id":"0lthyc2vb29I","trusted":true,"execution":{"iopub.status.busy":"2025-03-07T15:07:42.014339Z","iopub.execute_input":"2025-03-07T15:07:42.014621Z","iopub.status.idle":"2025-03-07T15:07:42.081507Z","shell.execute_reply.started":"2025-03-07T15:07:42.014600Z","shell.execute_reply":"2025-03-07T15:07:42.080705Z"}},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":"**EfficientNet V2**","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torchvision\nfrom torchvision import models\n\nefficientnet_v2 = models.efficientnet_v2_s(weights=models.EfficientNet_V2_S_Weights.IMAGENET1K_V1)\n\nfood_num_eff = efficientnet_v2.classifier[1].in_features\nefficientnet_v2.classifier[1] = nn.Linear(food_num_eff, 11)  \n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nefficientnet_v2 = efficientnet_v2.to(device)\n","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BMDGobcnh7Rb","outputId":"9df3f01a-e31e-4315-a963-0b0d94b5306f","trusted":true,"execution":{"iopub.status.busy":"2025-03-07T15:07:33.734907Z","iopub.execute_input":"2025-03-07T15:07:33.735207Z","iopub.status.idle":"2025-03-07T15:07:35.404085Z","shell.execute_reply.started":"2025-03-07T15:07:33.735183Z","shell.execute_reply":"2025-03-07T15:07:35.403318Z"}},"outputs":[{"name":"stderr","text":"Downloading: \"https://download.pytorch.org/models/efficientnet_v2_s-dd5fe13b.pth\" to /root/.cache/torch/hub/checkpoints/efficientnet_v2_s-dd5fe13b.pth\n100%|██████████| 82.7M/82.7M [00:01<00:00, 83.8MB/s]\n","output_type":"stream"}],"execution_count":6},{"cell_type":"markdown","source":"**MobileNet V3**","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torchvision\nfrom torchvision import models\n\nmobilenet_v3 = models.mobilenet_v3_large(weights=models.MobileNet_V3_Large_Weights.IMAGENET1K_V1)\n\nfood_num_mobile = mobilenet_v3.classifier[3].in_features\nmobilenet_v3.classifier[3] = nn.Linear(food_num_mobile, 11)  \n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmobilenet_v3 = mobilenet_v3.to(device)\n","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"B14-G-Gkh-JY","outputId":"f3b97080-814e-498f-e8e5-ebac0642ad3c","trusted":true,"execution":{"iopub.status.busy":"2025-03-07T15:07:39.694629Z","iopub.execute_input":"2025-03-07T15:07:39.694948Z","iopub.status.idle":"2025-03-07T15:07:40.159743Z","shell.execute_reply.started":"2025-03-07T15:07:39.694925Z","shell.execute_reply":"2025-03-07T15:07:40.158820Z"}},"outputs":[{"name":"stderr","text":"Downloading: \"https://download.pytorch.org/models/mobilenet_v3_large-8738ca79.pth\" to /root/.cache/torch/hub/checkpoints/mobilenet_v3_large-8738ca79.pth\n100%|██████████| 21.1M/21.1M [00:00<00:00, 85.4MB/s]\n","output_type":"stream"}],"execution_count":7},{"cell_type":"markdown","source":"### b. Fine-tune the model\nFine-tune the model. Experiment with different hyperparameter settings (learning rate, batch size, etc.) to optimize performance. Explain your tuning strategy.","metadata":{"id":"A_dlnp5mb29P"}},{"cell_type":"markdown","source":"**ShuffleNet**","metadata":{}},{"cell_type":"code","source":"import torch.optim as optim\nfrom torch.utils.data import DataLoader\nimport torch.nn.functional as F\nfrom torch import nn\n\noptimizer = optim.Adam(shufflenet.parameters(), lr=1e-4, weight_decay=1e-4)\n\nscheduler = optim.lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n\nnum_epochs = 10\n\nfor epoch in range(num_epochs):\n    shufflenet.train()\n    running_loss = 0.0\n    for i, lab in train_loader:\n        i, lab = i.to(device), lab.to(device)\n\n        optimizer.zero_grad()\n\n        outputs = shufflenet(i)\n        loss = F.cross_entropy(outputs, lab)\n\n        loss.backward()\n        optimizer.step()\n\n        running_loss += loss.item()\n\n    scheduler.step()\n\n    shufflenet.eval()\n    val_loss = 0.0\n    with torch.no_grad():\n        for i, lab in val_loader:\n            i, lab = i.to(device), lab.to(device)\n            outputs = shufflenet(i)\n            val_loss += F.cross_entropy(outputs, lab).item()\n\n    val_loss /= len(val_loader)\n    print(f\"Epoch {epoch+1}/{num_epochs}, Training Loss: {running_loss/len(train_loader)}, Validation Loss: {val_loss}\")\n","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":408},"id":"N-vuGsl3b29Q","outputId":"5cc78a74-0cb8-4bf2-d880-76291dd2067d","trusted":true,"execution":{"iopub.status.busy":"2025-03-07T15:16:52.756002Z","iopub.execute_input":"2025-03-07T15:16:52.756355Z","iopub.status.idle":"2025-03-07T15:33:03.918411Z","shell.execute_reply.started":"2025-03-07T15:16:52.756323Z","shell.execute_reply":"2025-03-07T15:33:03.917476Z"}},"outputs":[{"name":"stdout","text":"Epoch 1/10, Training Loss: 0.643575984779685, Validation Loss: 0.5480339040427848\nEpoch 2/10, Training Loss: 0.5131644405014693, Validation Loss: 0.45035087072324975\nEpoch 3/10, Training Loss: 0.4208368671075426, Validation Loss: 0.42166456880254877\nEpoch 4/10, Training Loss: 0.3556286305382028, Validation Loss: 0.4001387778531622\nEpoch 5/10, Training Loss: 0.30858215899432745, Validation Loss: 0.3836018298321438\nEpoch 6/10, Training Loss: 0.27670327578837045, Validation Loss: 0.39514666949226346\nEpoch 7/10, Training Loss: 0.2340304879140121, Validation Loss: 0.37874830495221196\nEpoch 8/10, Training Loss: 0.19494248260548006, Validation Loss: 0.3709305880108365\nEpoch 9/10, Training Loss: 0.18461875595200603, Validation Loss: 0.3661576414791246\nEpoch 10/10, Training Loss: 0.17464023715783283, Validation Loss: 0.3631447854589809\n","output_type":"stream"}],"execution_count":13},{"cell_type":"markdown","source":"**EfficientNet V2**","metadata":{}},{"cell_type":"code","source":"import torch.optim as optim\nfrom torch.utils.data import DataLoader\nfrom torchvision import datasets, transforms\n\ntransform = transforms.Compose([\n    transforms.Resize((224, 224)), \n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]) \n])\n\n\n\ntrain_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n\ncriterion = nn.CrossEntropyLoss()  \noptimizer = optim.Adam(efficientnet_v2.parameters(), lr=0.0001)  \n\n\nnum_epochs = 10\n\nfor epoch in range(num_epochs):\n    efficientnet_v2.train()\n    running_train_loss = 0.0\n    correct_train = 0\n    total_train = 0\n    \n    for i, lab in train_loader:\n        i, lab = i.to(device), lab.to(device)\n        \n        optimizer.zero_grad()\n        outputs = efficientnet_v2(i)\n        loss = criterion(outputs, lab)\n        loss.backward()\n        optimizer.step()\n        \n        running_train_loss += loss.item()\n        \n        _, predicted = torch.max(outputs, 1)\n        total_train += lab.size(0)\n        correct_train += (predicted == lab).sum().item()\n    \n    avg_train_loss = running_train_loss / len(train_loader)\n    \n    efficientnet_v2.eval()\n    running_val_loss = 0.0\n    correct_val = 0\n    total_val = 0\n    \n    with torch.no_grad():\n        for i, lab in val_loader:\n            i, lab = i.to(device), lab.to(device)\n            \n            outputs = efficientnet_v2(i)\n            loss = criterion(outputs, lab)\n            running_val_loss += loss.item()\n            \n            _, predicted = torch.max(outputs, 1)\n            total_val += lab.size(0)\n            correct_val += (predicted == lab).sum().item()\n    \n    avg_val_loss = running_val_loss / len(val_loader)\n    \n    print(f\"Epoch [{epoch+1}/{num_epochs}], Training Loss: {avg_train_loss:.4f}, Validation Loss: {avg_val_loss:.4f} \")\n         \n    \n","metadata":{"id":"jHQ6KSrgmJ-2","trusted":true,"execution":{"iopub.status.busy":"2025-03-07T15:39:52.075026Z","iopub.execute_input":"2025-03-07T15:39:52.075358Z","iopub.status.idle":"2025-03-07T16:13:38.120847Z","shell.execute_reply.started":"2025-03-07T15:39:52.075330Z","shell.execute_reply":"2025-03-07T16:13:38.119800Z"}},"outputs":[{"name":"stdout","text":"Epoch [1/10], Training Loss: 0.2296, Validation Loss: 0.2404 \nEpoch [2/10], Training Loss: 0.1199, Validation Loss: 0.2094 \nEpoch [3/10], Training Loss: 0.0762, Validation Loss: 0.2374 \nEpoch [4/10], Training Loss: 0.0671, Validation Loss: 0.2447 \nEpoch [5/10], Training Loss: 0.0535, Validation Loss: 0.2618 \nEpoch [6/10], Training Loss: 0.0372, Validation Loss: 0.2635 \nEpoch [7/10], Training Loss: 0.0456, Validation Loss: 9.7429 \nEpoch [8/10], Training Loss: 0.0357, Validation Loss: 0.6203 \nEpoch [9/10], Training Loss: 0.0369, Validation Loss: 0.2145 \nEpoch [10/10], Training Loss: 0.0354, Validation Loss: 2.0196 \n","output_type":"stream"}],"execution_count":20},{"cell_type":"markdown","source":"**MobileNet V3**","metadata":{}},{"cell_type":"code","source":"import torch.optim as optim\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader\n\noptimizer = optim.Adam(mobilenet_v3.parameters(), lr=1e-4, weight_decay=1e-4)\nscheduler = optim.lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n\nnum_epochs = 10\n\nmobilenet_v3 = mobilenet_v3.to(device)  \n\nfor epoch in range(num_epochs):\n    mobilenet_v3.train()\n    running_loss = 0.0\n    for i, lab in train_loader:\n        i, lab = i.to(device), lab.to(device)\n\n        optimizer.zero_grad()\n\n        outputs = mobilenet_v3(i)\n        loss = F.cross_entropy(outputs, lab)\n\n        loss.backward()\n        optimizer.step()\n\n        running_loss += loss.item()\n\n    scheduler.step()\n\n    mobilenet_v3.eval()\n    val_loss = 0.0\n    with torch.no_grad():\n        for i, lab in val_loader:\n            i, lab = i.to(device), lab.to(device)\n            outputs = mobilenet_v3(i)\n            val_loss += F.cross_entropy(outputs, lab).item()\n\n    val_loss /= len(val_loader)\n    print(f\"Epoch {epoch+1}/{num_epochs}, Training Loss: {(running_loss/len(train_loader)):.4f}, Validation Loss: {val_loss:.4f}\")\n","metadata":{"id":"_amwre-qmd8B","trusted":true,"execution":{"iopub.status.busy":"2025-03-07T16:15:12.438206Z","iopub.execute_input":"2025-03-07T16:15:12.438523Z","iopub.status.idle":"2025-03-07T16:33:17.955494Z","shell.execute_reply.started":"2025-03-07T16:15:12.438497Z","shell.execute_reply":"2025-03-07T16:33:17.954571Z"}},"outputs":[{"name":"stdout","text":"Epoch 1/10, Training Loss: 0.8798, Validation Loss: 0.4516\nEpoch 2/10, Training Loss: 0.3443, Validation Loss: 0.3555\nEpoch 3/10, Training Loss: 0.2271, Validation Loss: 0.3496\nEpoch 4/10, Training Loss: 0.1564, Validation Loss: 0.3903\nEpoch 5/10, Training Loss: 0.1073, Validation Loss: 0.3477\nEpoch 6/10, Training Loss: 0.0879, Validation Loss: 0.3612\nEpoch 7/10, Training Loss: 0.0662, Validation Loss: 0.3659\nEpoch 8/10, Training Loss: 0.0370, Validation Loss: 0.3276\nEpoch 9/10, Training Loss: 0.0318, Validation Loss: 0.3221\nEpoch 10/10, Training Loss: 0.0267, Validation Loss: 0.3264\n","output_type":"stream"}],"execution_count":21},{"cell_type":"markdown","source":"**Fine Tuning Strategy**\n\n1. Freeze Early Layers: First freeze the earlier layers of the pre-trained model; these layers have learned low-level features such as edges and textures, which will generally be valuable for any dataset. The only fine-tuning will therefore occur at later layers.\n\n\n2. Unfreeze Layers Gradually: After a few epochs on the final training layers, unfreeze the early layers to enable further refinement of the model to the Food-11-specific features\n\n3. Learning Rate: Fine-tuning should be done under lower learning rates, since higher learning rates might burn up the relevant learned features that the pre-trained model has.\n\n4. Optimizer: Adam or SGD can act as the optimizer. Adam is often a good choice for transfer learning, as it can adapt the learning rate based on the gradients.\n\n5. Batch Size: Test different batch sizes. A common size batch could be 32 or 64, this can be adjusted according to Maximum GPU memory available and performance, as usually smaller batch sizes create much noisier gradients, while larger sizes may improve stability.\n\n6. Learning Rate Scheduler: Learning rate scheduler such as StepLR or ReduceLROnPlateau would dynamically adjust the learning rate during the training.","metadata":{"id":"B4UyN2Ltb29R"}},{"cell_type":"markdown","source":"### c. Evaluate the performance of each fine-tuned model\nEvaluate the performance of each fine-tuned model on the Food-11 dataset.","metadata":{"id":"IrJr5d--b29R"}},{"cell_type":"markdown","source":"**ShuffleNet**","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn.functional as F\n\nshufflenet.eval()\n\nnum_epochs = 10  \ntest_results = []\n\nfor epoch in range(num_epochs):\n    test_loss = 0.0\n    correct = 0\n    total = 0\n\n    with torch.no_grad(): \n        for i, lab in test_loader:\n            i, lab = i.to(device), lab.to(device)\n\n            outputs = shufflenet(i)\n\n            loss = F.cross_entropy(outputs, lab)\n            test_loss += loss.item()\n\n            _, predicted = torch.max(outputs, 1)  \n            total += lab.size(0)\n            correct += (predicted == lab).sum().item()\n\n    avg_test_loss = test_loss / len(test_loader)\n    test_acc = 100 * correct / total\n\n    print(f\"Epoch {epoch+1}/{num_epochs}, Test Loss: {avg_test_loss:.4f}, Test Accuracy: {test_acc:.2f}%\")\n\n    test_results.append((epoch + 1, avg_test_loss, test_acc))\n","metadata":{"id":"78Ck8Ab8b29X","trusted":true,"execution":{"iopub.status.busy":"2025-03-07T16:41:18.635045Z","iopub.execute_input":"2025-03-07T16:41:18.635394Z","iopub.status.idle":"2025-03-07T16:45:30.972959Z","shell.execute_reply.started":"2025-03-07T16:41:18.635361Z","shell.execute_reply":"2025-03-07T16:45:30.972016Z"}},"outputs":[{"name":"stdout","text":"Epoch 1/10, Test Loss: 0.3017, Test Accuracy: 89.84%\nEpoch 2/10, Test Loss: 0.3017, Test Accuracy: 89.84%\nEpoch 3/10, Test Loss: 0.3017, Test Accuracy: 89.84%\nEpoch 4/10, Test Loss: 0.3017, Test Accuracy: 89.84%\nEpoch 5/10, Test Loss: 0.3017, Test Accuracy: 89.84%\nEpoch 6/10, Test Loss: 0.3017, Test Accuracy: 89.84%\nEpoch 7/10, Test Loss: 0.3017, Test Accuracy: 89.84%\nEpoch 8/10, Test Loss: 0.3017, Test Accuracy: 89.84%\nEpoch 9/10, Test Loss: 0.3017, Test Accuracy: 89.84%\nEpoch 10/10, Test Loss: 0.3017, Test Accuracy: 89.84%\n","output_type":"stream"}],"execution_count":22},{"cell_type":"markdown","source":"**EfficientNet V2**","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn.functional as F\n\nefficientnet_v2.eval()\n\nnum_epochs = 10  \ntest_results = []\n\nfor epoch in range(num_epochs):\n    test_loss = 0.0\n    acc = 0\n    total = 0\n\n    with torch.no_grad():  \n        for i, lab in test_loader:\n            i, lab = i.to(device), lab.to(device)\n\n            outputs = efficientnet_v2(i)  \n\n            loss = F.cross_entropy(outputs, lab)\n            test_loss += loss.item()\n\n            _, predicted = torch.max(outputs, 1)  \n            total += lab.size(0)\n            acc += (predicted == lab).sum().item()\n\n    avg_test_loss = test_loss / len(test_loader)\n    test_acc = 100 * acc / total\n\n    print(f\"Epoch {epoch+1}/{num_epochs}, Test Loss: {avg_test_loss:.4f}, Test Accuracy: {test_acc:.2f}%\")\n\n    test_results.append((epoch + 1, avg_test_loss, test_acc))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-07T16:48:55.331433Z","iopub.execute_input":"2025-03-07T16:48:55.331752Z","iopub.status.idle":"2025-03-07T16:53:58.266583Z","shell.execute_reply.started":"2025-03-07T16:48:55.331725Z","shell.execute_reply":"2025-03-07T16:53:58.265574Z"}},"outputs":[{"name":"stdout","text":"Epoch 1/10, Test Loss: 1.0026, Test Accuracy: 95.10%\nEpoch 2/10, Test Loss: 1.0026, Test Accuracy: 95.10%\nEpoch 3/10, Test Loss: 1.0026, Test Accuracy: 95.10%\nEpoch 4/10, Test Loss: 1.0026, Test Accuracy: 95.10%\nEpoch 5/10, Test Loss: 1.0026, Test Accuracy: 95.10%\nEpoch 6/10, Test Loss: 1.0026, Test Accuracy: 95.10%\nEpoch 7/10, Test Loss: 1.0026, Test Accuracy: 95.10%\nEpoch 8/10, Test Loss: 1.0026, Test Accuracy: 95.10%\nEpoch 9/10, Test Loss: 1.0026, Test Accuracy: 95.10%\nEpoch 10/10, Test Loss: 1.0026, Test Accuracy: 95.10%\n","output_type":"stream"}],"execution_count":23},{"cell_type":"markdown","source":"**MobileNet V3**","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn.functional as F\n\nmobilenet_v3.eval()\n\nnum_epochs = 10  \ntest_results = []\n\nfor epoch in range(num_epochs):\n    test_loss = 0.0\n    correct = 0\n    total = 0\n\n    with torch.no_grad():  \n        for i, lab in test_loader:\n            i, lab = i.to(device), lab.to(device)\n\n            outputs = mobilenet_v3(i)\n\n            loss = F.cross_entropy(outputs, lab)\n            test_loss += loss.item()\n\n            _, predicted = torch.max(outputs, 1)  \n            total += lab.size(0)\n            correct += (predicted == lab).sum().item()\n\n    avg_test_loss = test_loss / len(test_loader)\n    test_accuracy = 100 * correct / total\n\n    print(f\"Epoch {epoch+1}/{num_epochs}, Test Loss: {avg_test_loss:.4f}, Test Accuracy: {test_accuracy:.2f}%\")\n\n    test_results.append((epoch + 1, avg_test_loss, test_accuracy))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-07T16:53:58.267744Z","iopub.execute_input":"2025-03-07T16:53:58.268004Z","iopub.status.idle":"2025-03-07T16:57:56.923213Z","shell.execute_reply.started":"2025-03-07T16:53:58.267971Z","shell.execute_reply":"2025-03-07T16:57:56.922393Z"}},"outputs":[{"name":"stdout","text":"Epoch 1/10, Test Loss: 0.2717, Test Accuracy: 92.35%\nEpoch 2/10, Test Loss: 0.2717, Test Accuracy: 92.35%\nEpoch 3/10, Test Loss: 0.2717, Test Accuracy: 92.35%\nEpoch 4/10, Test Loss: 0.2717, Test Accuracy: 92.35%\nEpoch 5/10, Test Loss: 0.2717, Test Accuracy: 92.35%\nEpoch 6/10, Test Loss: 0.2717, Test Accuracy: 92.35%\nEpoch 7/10, Test Loss: 0.2717, Test Accuracy: 92.35%\nEpoch 8/10, Test Loss: 0.2717, Test Accuracy: 92.35%\nEpoch 9/10, Test Loss: 0.2717, Test Accuracy: 92.35%\nEpoch 10/10, Test Loss: 0.2717, Test Accuracy: 92.35%\n","output_type":"stream"}],"execution_count":24},{"cell_type":"markdown","source":"### d. Compare the results obtained with the different pre-trained models\nDiscuss which model performed best and analyze the reasons for the observed differences in performance.","metadata":{"id":"RZOo9MNPb29Z"}},{"cell_type":"markdown","source":"**Resulting summary table of model performances**\n\n\n| Model           | Best Validation Loss | Test Loss | Test Accuracy (%) | Final Training Loss |\n|---------------|--------------------|-----------|----------------|------------------|\n| ShuffleNet V2  | 0.3631             | 0.3017    | 89.84          | 0.1746           |\n| EfficientNet V2 | 0.2145             | 1.0026    | 95.10          | 0.0354           |\n| MobileNet V3   | 0.3221             | 0.2717    | 92.35          | 0.0267           |\n","metadata":{}},{"cell_type":"markdown","source":"**Best Performing Model: EfficientNetV2 with 95.10% accuracy**\n\n**Analysis:** \n\nOverall, EfficientNetV2 showed best results as it had the highest accuracy at 95.10% and validated performance with a loss of 0.2145, thus making it best model for Food-11 classification.\n\nMobileNetV3 also had good values in accuracy (92.35%) and lowest test loss (0.2717), making it economical when deployed.\n\nAlthough ShuffleNet V2 gives high efficiency, it has small accuracy at only 89.84% and high test loss making it the least effective model.\n","metadata":{}},{"cell_type":"markdown","source":"**Reasons for model performance**\n\n**EfficientNet V2**\n1. The compound scaling improves the feature extraction by optimizing the network in the depth, width, and resolution of the representation.\nThis helps the model to distinguish minor differences in food images thus improving the classification performance.\n2. Dropout, Stochastic Depth, and Squeeze-and-Excitation layers prevent the model from overfitting and maintain the high accuracy of validation and test sets.\n\n3. EfficientNetV2-trained large dataset (ImageNet1K) thereby make more possible transfer-learning and hence better for adaptation to the Food-11 dataset.\n\n**MobileNetV3**\n1. The given configuration utilizes Depth wise Separable Convolutions to produce a reduced computation. Furthermore, it is indeed easier for training and inference for real-time food classification apps.\n2. Highly strong generalization with lowest test loss implies that lesser resources are used for computation; thus, it performs well for new as well as unseen test data.\n\n**ShuffleNetV2** \n1. It is a lightweight model but did not manage to perform well in comparison with the depth architectures. It might have lost the important details in food images.\n2. Higher test loss denotes comparatively weak generalization against MobileNetV3 and EfficientNetV2.\n","metadata":{}},{"cell_type":"markdown","source":"## Step 3: References\nInclude details on all the resources used to complete this part.","metadata":{"id":"OxMUdjYrb29a"}},{"cell_type":"markdown","source":"1. https://pytorch.org/hub/pytorch_vision_shufflenet_v2/\n2. https://medium.com/aimonks/shufflenet-revolutionizing-mobile-deep-learning-e15237239f47\n3. https://pytorch.org/vision/main/models/efficientnetv2.html\n4. https://medium.com/towards-data-science/efficientnetv2-faster-smaller-and-higher-accuracy-than-vision-transformers-98e23587bf04\n5. https://medium.com/@RobuRishabh/understanding-and-implementing-mobilenetv3-422bd0bdfb5a\n6. https://pytorch.org/vision/main/models/mobilenetv3.html","metadata":{"id":"ewnubHozb29h"}}]}